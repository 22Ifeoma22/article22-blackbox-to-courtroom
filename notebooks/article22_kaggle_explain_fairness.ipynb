# Ensure folder exists
New-Item -ItemType Directory -Force -Path notebooks | Out-Null

# Create the notebook (compact JSON)
$nb = @'
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Article 22 — Explainability & Fairness (Kaggle-ready)\\n",
    "**Steps**: install deps → load German Credit → add lag features → train RF → SHAP → Fairlearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install shap fairlearn scikit-learn pandas numpy matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pandas as pd, numpy as np, matplotlib.pyplot as plt\\n",
    "from sklearn.model_selection import train_test_split\\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\\n",
    "from sklearn.compose import ColumnTransformer\\n",
    "from sklearn.pipeline import Pipeline\\n",
    "from sklearn.impute import SimpleImputer\\n",
    "from sklearn.ensemble import RandomForestClassifier\\n",
    "from sklearn.metrics import roc_auc_score, classification_report\\n",
    "import shap\\n",
    "from fairlearn.metrics import MetricFrame, selection_rate, false_negative_rate, false_positive_rate\\n",
    "\\n",
    "# --- Load a German Credit CSV attached in Kaggle or placed locally ---\\n",
    "CANDIDATES = [\\n",
    "  '/kaggle/input/german-credit-data-with-risk/german_credit_data.csv',\\n",
    "  'data/german_credit_data.csv',\\n",
    "  'german_credit_data.csv'\\n",
    "]\\n",
    "df = None\\n",
    "for p in CANDIDATES:\\n",
    "    if os.path.exists(p):\\n",
    "        df = pd.read_csv(p)\\n",
    "        print('Loaded:', p, 'shape:', df.shape)\\n",
    "        break\\n",
    "if df is None:\\n",
    "    raise FileNotFoundError('Place the German Credit CSV at one of: ' + ', '.join(CANDIDATES))\\n",
    "\\n",
    "# --- Target & basic harmonisation ---\\n",
    "tcol = next((c for c in df.columns if c.lower() in ['risk','target','class']), None)\\n",
    "y = df[tcol].astype(str).str.lower().map({'bad':1,'good':0,'1':1,'0':0})\\n",
    "df = df.copy(); df['y']=y\\n",
    "\\n",
    "# --- Simple lag features (pseudo-temporal) ---\\n",
    "df = df.sort_values(df.columns[0]).reset_index(drop=True)\\n",
    "num_cols = [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c]) and c!='y'][:6]\\n",
    "for c in num_cols:\\n",
    "    df[f'lag_{c}'] = df[c].shift(1).fillna(df[c].median())\\n",
    "\\n",
    "# --- Feature splits ---\\n",
    "X = df.drop(columns=[tcol,'y'], errors='ignore'); y = df['y']\\n",
    "cat = [c for c in X.columns if df[c].dtype=='object']\\n",
    "num = [c for c in X.columns if c not in cat]\\n",
    "\\n",
    "pre = ColumnTransformer([\\n",
    "    ('cat', Pipeline([('imp',SimpleImputer(strategy='most_frequent')), ('ohe',OneHotEncoder(handle_unknown='ignore'))]), cat),\\n",
    "    ('num', Pipeline([('imp',SimpleImputer(strategy='median')), ('sc',StandardScaler())]), num)\\n",
    "])\\n",
    "\\n",
    "Xtr, Xte, ytr, yte = train_test_split(X,y,test_size=0.3,random_state=42,stratify=y)\\n",
    "rf = Pipeline([('pre',pre), ('clf',RandomForestClassifier(n_estimators=300,random_state=42))])\\n",
    "rf.fit(Xtr,ytr)\\n",
    "proba = rf.predict_proba(Xte)[:,1]\\n",
    "pred  = (proba>=0.5).astype(int)\\n",
    "print('ROC-AUC:', round(roc_auc_score(yte,proba),3))\\n",
    "print(classification_report(yte,pred,digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- SHAP global summary ---\\n",
    "Xt_tr = rf.named_steps['pre'].fit_transform(Xtr)\\n",
    "Xt_te = rf.named_steps['pre'].transform(Xte)\\n",
    "from sklearn.ensemble import RandomForestClassifier as RFC\\n",
    "est = RFC(n_estimators=300, random_state=42).fit(Xt_tr, ytr)\\n",
    "expl = shap.TreeExplainer(est)\\n",
    "sv = expl.shap_values(Xt_te)\\n",
    "ohe = rf.named_steps['pre'].named_transformers_['cat'].named_steps['ohe']\\n",
    "feat = list(ohe.get_feature_names_out(rf.named_steps['pre'].transformers_[0][2])) + [c for c in rf.named_steps['pre'].transformers_[1][2]]\\n",
    "shap.summary_plot(sv[1], Xt_te, feature_names=feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Fairness by Sex (if present) ---\\n",
    "sex_col = next((c for c in df.columns if c.lower() in ['sex','personal_status_and_sex']), None)\\n",
    "if sex_col:\\n",
    "    y_pred = (rf.predict_proba(Xte)[:,1] >= 0.5).astype(int)\\n",
    "    sf = df.loc[Xte.index, sex_col].astype(str).str.lower().map({'m':'male','male':'male','f':'female','female':'female'}).fillna('unknown')\\n",
    "    frame = MetricFrame(metrics={'accuracy':__import__('sklearn').metrics.accuracy_score,\\n",
    "                                 'selection_rate':selection_rate,\\n",
    "                                 'false_positive_rate':false_positive_rate,\\n",
    "                                 'false_negative_rate':false_negative_rate},\\n",
    "                        y_true=yte, y_pred=y_pred, sensitive_features=sf)\\n",
    "    display(frame.by_group)\\n",
    "else:\\n",
    "    print('Sex column not found; skip fairness-by-sex.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"},
  "language_info": {"name": "python", "version": "3.x"}
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
'@

$path = ".\notebooks\article22_kaggle_explain_fairness.ipynb"
$nb | Out-File -FilePath $path -Encoding utf8

# Commit & push
git add $path
git commit -m "Add Kaggle-ready explainability & fairness notebook"
git push
